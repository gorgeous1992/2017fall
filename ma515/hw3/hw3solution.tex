\documentclass[12pt]{article}
 \usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,amsfonts}
\usepackage{color}
\usepackage{mathtools, eucal}
\usepackage{xparse}
\usepackage{romannum}
 
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

 
\begin{document}
 
%\renewcommand{\qedsymbol}{\filledbox}
%Good resources for looking up how to do stuff:
%Binary operators: http://www.access2science.com/latex/Binary.html
%General help: http://en.wikibooks.org/wiki/LaTeX/Mathematics
%Or just google stuff
 
\title{MA 515 Homework 3}
\author{Zheming Gao}
\maketitle

\section*{Problem 1}

\begin{enumerate}
\item [(a)]

It is not a norm because it violates property (\romannum{2}). Let $a = -1, x = 1$. Then $||ax|| = ||-1|| = 2$. However, $|a|||x|| = 1$.

\item [(b)]
It is a norm.

\begin{proof}

Check the first one. If $||f|| = 0$, then $\sup_{t\geqslant 0} e^{\lambda t}|f(t)| = 0$. Since for any $t\geqslant 0$, $e^{\lambda t}|f(t)| \geqslant 0$ and $e^{\lambda t} > 0$, we have  $|f(t)| = 0, \forall t\geqslant 0$. This is to say $f \equiv 0$ on its domain. For the other direction, $||f|| = 0$ is true when $f = 0$.

For (\romannum 2), $\forall \alpha \in \mathbb{R}$,

$$
||\alpha x|| = \sup_{t\geqslant 0} e^{\lambda t}|\alpha f(t)| = \sup_{t\geqslant 0} e^{\lambda t}|\alpha|\cdot|f(t)| = |\alpha| \sup_{t\geqslant 0} e^{\lambda t}|f(t)| = |\alpha|\cdot ||x||.
$$

To check the triangle inequality, we pick $f, g \in X$,

$$
\begin{aligned}
||f + g|| = \sup_{t\geqslant 0} e^{\lambda t}|f(t) + g(t)| & \leqslant \sup_{t\geqslant 0} e^{\lambda t}\left(|f(t)| + |g(t)|\right)\\
& \leqslant \sup_{t\geqslant 0} e^{\lambda t}|f(t)| + \sup_{t\geqslant 0} e^{\lambda t}|g(t)| = ||f|| + ||g||.
\end{aligned}
$$

In conclusion, it is a norm.

\end{proof}

\item [(c)]

It is a norm.

\begin{proof}

We have shown that for $\ell^p$ space, $||x||_p = (\sum_{i=1}^{+\infty} |x_i|^p)^{1/p} $ is a norm for $1\leqslant p \leqslant +\infty$. Then, truncate it for only first two entries. Consider set $S = \{ x \in \ell^p \| x = (x_1, x_2, 0, \dots),  x_1, x_2 \in \mathbb{R}\}$. We have $||x||_p = ||x||, \forall x\in S$. Since $||\cdot||_p$ is a norm on $\ell^p$, it must be a norm on $S\subset \ell^p$ as $0\in S$. Then we conclude that $||\cdot||$ is a norm on $\mathbb{R}^2$.

\end{proof}

\item [(d)]

It is not a norm. Consider $x = (1, 0), y = (0, 1)$. Then $||x+y|| = ||(1,1)|| = 2^{1/p} > 2$. However, $||x|| + ||y|| = 2 < ||x + y||$. This breaks the triangle inequality.


\end{enumerate}

\section*{Problem 2}

\begin{proof}
\ 

\begin{enumerate}
\item [Step 1]

We are going to show $||(\cdot, \cdot)||$ is a norm on $X\times Y$. 

If $||(x, y)|| = 0$, then $\max \{ ||x||_X, ||y||_Y \} = 0$, which implies that $||x||_X = ||y||_Y = 0$. Hence, $x = y = 0$. The other direction is obvious. 

To check the second property, take arbitrarily $\alpha \in \mathbb R$ and we have 

$$
\begin{aligned}
||\alpha (x, y)|| = ||(\alpha x, \alpha y)|| & = \max \{ ||\alpha x||_X, ||\alpha y||_Y \}\\
& = \max \{ |\alpha | \cdot ||x||_X, |\alpha | \cdot ||y||_Y \}\\ & = |\alpha|\max \{ ||x||_X, ||y||_Y \} = |\alpha| \cdot ||(x, y)||.
\end{aligned}
$$

For triangle inequality, take arbitrarily $(x, y), (z, w) \in X\times Y$, we have 

$$
\begin{aligned}
||(x, y) + (z, w)|| = ||(x+z, y+w)|| & = \max\{ ||x+z||_X, ||y+w||_Y \} \\
& \leqslant \max\{ ||x||_X + ||z||_X, ||y||_Y + ||w||_Y \} \\
& = \max\{ ||x+||_X, ||y||_Y \} + \max\{ ||z||_X, ||w||_Y \} \\
& = ||(x, y)|| + ||(z, w)||.
\end{aligned}
$$

Hence, $||(\cdot, \cdot)||$ is a norm on $X\times Y$. 

\item [Step 2]

We need to prove that $X\times Y$ is complete. Take any Cauchy sequence $\{z_n = (x_n, y_n)\}_{n\in\mathbb N} \subset X\times Y$. $\forall \epsilon >0$, there exists $N > 0$ such that 
$$ 
||z_n - z_m|| = ||(x_n, y_n) - (x_m, y_m)|| < \epsilon, \quad \forall n, m > N.
$$

This is equivalent to $\max\{||x_n - x_m||_X, ||y_n - y_m||_Y\} < \epsilon \forall n, m > N$ and also implies that 

$$
||x_n - x_m||_X < \epsilon, \quad ||y_n - y_m||_Y < \epsilon, 
\forall n, m > N.
$$

Hence we conclude that $\{x_n\}$ and $\{y_n\}$ are both Cauchy sequences on $X$ and $Y$ respectively. Since $(X, ||\cdot||_X)$ and $(Y, ||\cdot||_Y)$ are Banach spaces, $\{x_n\}$ converges on $X$ and $\{y_n\}$ converges on $Y$. Let $\lim_{n\rightarrow +\infty} x_n = x$, $\lim_{n\rightarrow +\infty} y_n = y$, and $z = (x, y)\in X\times Y$. Then,
$$ 
||z_n - z|| = ||(x_n, y_n) - (x, y)|| = \max\{||x_n - x||_X, ||y_n - y||_Y\}< \epsilon, \quad \forall n > N.
$$

Hence, $\{z_n\}$ converges to on $z\in X\times Y$. This proves the claim.




\end{enumerate}


\end{proof}


\section*{Problem 3}

\begin{enumerate}
\item [(a)]

\begin{proof}

Firstly, we need to show that $d_f$ is a metric on $X$. $\forall x, y\in X$, $d_f(x, y) = f(||x - y||_X) \geqslant 0$ holds due to the property of $f$. And $d_f(x, x) = f(0) = 0$. Also, $d_f(x, y) = d_f(y, x)$ is obvious since $||x - y||_X = ||y - x||_X$. For triangle inequality, we need to use the facts that $f$ is increasing and \romannum 2,
$$
\begin{aligned}
d_f(x, y) + d_f(y, z) & = f(||x-y||_X) + f(||y-z||_X)  \geqslant f(||x-y||_X + ||y-z||_X)\\
& \geqslant f(||x-y + y-z||_X) = f(||x-z||_X) = d_f(x, z).
\end{aligned}
$$
Hence, $d_f$ is a metric well-defined on $X$.

Next, we need to show $(X, d_f)$ is complete. Take any Cauchy sequence $\{x_n\}_n\in\mathbb N$ from $(X, d_f)$ and it is enough to show it converges in $X$. $\forall \epsilon > 0$, there exists $N > 0$ such that $d_f(x_n, x_m) = f(||x_n - x_m||_X) < \epsilon$, for all $n, m > N$. Since $f$ is an incr easing continuous function from $\mathbb R_+$ to $\mathbb R_+$, then there exists an increasing continuous inverse function $f^{-1}$ of $f$ such that $f^{-1}(d_f(x, y)) = ||x - y||_X$. Hence,

$$
||x_m - x_n||_X = f^{-1}(d_f(x_m, x_n)) < f^{-1}(\epsilon) \rightarrow 0,\  \text{as} \quad \epsilon \rightarrow 0, \forall n, m > N
$$

Hence, $\{x_n\}$ is also a Cauchy sequence on $(X, ||\cdot||_X)$ and so it converges in $X$ in norm $||\cdot||_X$. Let $x_n\rightarrow x\in X$ in $||\cdot||_X$. Then we have $\lim_{n\rightarrow +\infty } ||x_n - x||_X = 0$. With the continuity of $f$, we know $\lim_{n\rightarrow +\infty } f(||x_n - x||_X) = 0$. And this improves that $x_n\rightarrow x$ in distance $d_f$. Hence $\{x_n\}$ is convergent on $(X, d_f)$.

In conclusion, $(X, d_f)$ is a complete metric space.

\end{proof}

\item [(b)]

\begin{proof}

Take arbitrarily $x, y\in B_f(0, 1)$ and $\alpha\in(0, 1)$. Then $z = \alpha x + (1- \alpha)y\in X$. It is enough to show that $d_f(0, z) < 1$. Use property (\romannum 2) and the property given in (b), we have the following,

$$
\begin{aligned}
d_f(0, z) = f(||z||_X) & \leqslant f(\alpha||x||_X + (1-\alpha)||y||_X) \\
& \leqslant f(\alpha||x||_X) + f((1-\alpha)||y||_X) \\
& \alpha f(||x||_X) + (1-\alpha)f(||y||_X)\\
& = \alpha d_f(0, x) + (1-\alpha) d_f(0, y) < \alpha + 1 - \alpha = 1.
\end{aligned}
$$ 

This proves that $z \in B_f(0,1)$ and then $B_f(0, 1)$ is convex.


\end{proof}


\end{enumerate}


\section*{Problem 4}

\begin{proof}

"$\Leftarrow$", if absolute convergence of any sequence $\{x_n\}$ implies the convergence of this sequence, then $(X, ||\cdot||)$ is complete. Take $\{x_n\}_{n\in\mathbb N}$ as a Cauchy sequence on $X$, then from the definition we can always find a subsequence  $\{x_{n_k}\}$ such that $||x_{n_k} - x_{n_{k+1}}|| < 2^{-k}$. Let $y_k = x_{n_k} - x_{n_{k+1}}$ and we know for each $N>0$, 

$$
\sum_{k=1}^N ||y_k|| = \sum_{k=1}^N ||x_{n_k} - x_{n_{k+1}}|| <  \sum_{k=1}^N \frac{1}{2^k}.
$$

Since $N$ is arbitrarily chosen, we take the limits on both sides such that $\sum_{k=1}^{+\infty} ||y_k|| < +\infty$. This implies the convergent of $\{y_k\}$, i.e, $\lim_{N\rightarrow +\infty} \sum_{k=1}^N y_k = \sum_{k=1}^{+\infty} y_k < +\infty$.

Suppose $S_y = \lim_{N\rightarrow +\infty} \sum_{k=1}^N y_k$, then 
$$
S_y = \lim_{N\rightarrow +\infty} \sum_{k=1}^N y_k =\lim_{N\rightarrow +\infty} x_{n_1} - x_{n_{N+1}}.
$$

which implies that $\{x_{n_k}\}$ converges to $x_{n_1} - S_y$. Since it is a subsequence of Cauchy sequence $\{x_n\}$, $\{x_n\}$ also converges. This proves the completeness of $X$.

\vspace{15mm}

"$\Rightarrow$". Conversely, with Banach space $(X, ||\cdot||)$, and the absolutely convergent $\{x_n\}$ has sum $\sum_{n=1}^{+\infty} ||x_n|| < +\infty$, we need to prove the infinite sum of $\{x_n\}$ is also finite.

Since $\sum_{n=1}^{+\infty} ||x_n|| < +\infty$, for any $\epsilon > 0$, there exists $N_0>0$ such that $\sum_{n=N_0 + 1}^{+\infty} ||x_n|| < \epsilon$. Let $z_k = \sum_{n=1}^{k} x_n \in X$, $\{z_k\}$ is a Cauchy sequence because for any $\epsilon > 0$, take $N = N_0$, then

$$
||z_p, z_q|| \leqslant ||\sum_{n=q+1}^p x_n|| \leqslant \sum_{n=q+1}^p||x_n|| \leqslant \sum_{n=N+1}^{+\infty}||x_n|| < \epsilon, \quad \forall p, q > N.
$$

Then $\{z_k\}$ converges in Banach space $(X, ||\cdot||)$. 

In conclusion, we proved the claim.

\end{proof}


\section*{Problem 5}

\begin{proof}

For any $\epsilon>0$, and $\forall x\in \ell^p$, we need to find $e \in V$ such that $||e-x||_p < \epsilon$.

Since $x\in\ell^p$, we know $\sum_{i=1}^{+\infty} |x_i|^p < +\infty$ and this implies $\exists N$ such that $\sum_{i=N+1}^{+\infty} |x_i|^p < \epsilon^p$. Hence, we may let 

$$
e = \sum_{i=1}^{N} x_i e_i. 
$$

It is clear that $e\in V$ and check the distance between $e$ and $x$.

$$
||e-x||_p = \left( \sum_{i=1}^{N} |x_i -x_i|^p + \sum_{i=N+1}^{+\infty} |x_i|^p \right)^{1/p} = \left( \sum_{i=N+1}^{+\infty} |x_i|^p \right)^{1/p}<\epsilon.
$$

Hence, $V$ is dense in $\ell^p$.

\end{proof}

\section*{Problem 6}

\begin{proof}

We need to show for any $x\in X$, $\forall \epsilon > 0$, there exists $\delta >0$ such that $|f_n(x) - f_n(y)| < \epsilon$ holds, $\forall y\in B(x, \delta), f_n\in \{f_k\}_{k\in\mathbb N}$. 

Take arbitrary $x \in X$. Since $f$ is continuous on $X$, $\forall \epsilon > 0$, there exists $\delta_x > 0$ such that $|f(x) - f(y)| < \epsilon/3$ holds for any $y\in B(x, \delta_x)$. 

Also, the sequence of functions $\{f_n\} \rightarrow f$ uniformly, there exists $N>0$ such that $|f_n(x) - f(x)| < \epsilon /3$ holds for all $n>N$. By triangle inequality, we have 

$$
\begin{aligned}
|f_n(x) - f_n(y)| & \leqslant |f_n(x) - f(x)| + |f(x) - f(y)| + |f(y) - f_n(y)| \\
& < \epsilon/3 + \epsilon/3 + \epsilon/3 = \epsilon, \qquad \forall n>N, \forall y\in B(x, \delta_x).
\end{aligned}
$$

For each index  $i\in\{1, 2, \dots, N\}$, there exists $\delta_{i_x} > 0$ such that $|f_i(x) - f_i(y)|<\epsilon/3$ for any $y\in B(x, \delta_{i_x})$. We may let $\Delta_x = \min\{\delta_{1_x}, \dots, \delta_{N_x}, \delta_x\}$. Hence, for any $f_n \in \{f_k\}_{k\in\mathbb N}$, $|f_n(x) - f_n(x)|<\epsilon$ holds for all $y\in B(x, \Delta_x)$.

Hence, $\{f_k\}_{k\in\mathbb N}$ is equicontinuous on $X$, since it is equicontinuous at each $x\in X$.

\end{proof}

\section*{Problem 7}

\begin{enumerate}
\item [(a)]

\begin{proof}

To prove it is a normed vector space, we need to show that $0 \in C^{\alpha} ([a, b])$, $C^{\alpha} ([a, b])$ is closed under linear operations and $||\cdot||_\alpha$ is properly defined.

Firstly, $0\in C^\alpha([a, b])$ is obvious. For linearity, $\forall k\in \mathbb R$, $\forall f$ and $g\in C^\alpha([a, b])$,  we have 
$$
\frac{|kf(x) - kf(y)|}{|x-y|^\alpha} \leqslant |k|\cdot C.
$$

$$
\frac{|(f+g)(x) - (f+g)(y)|}{|x-y|^\alpha} \leqslant \frac{|f(x)-f(y)|}{|x-y|^\alpha} + \frac{|g(x)-g(y)|}{|x-y|^\alpha} \leqslant  2C.
$$

Hence, $C^{\alpha} ([a, b])$ is a vector space. Next we need to show that the norm is properly defined.

To begin with, $||f||_\alpha =0 \Rightarrow \max\left\{ |f(x)| + |f(x) - f(y)|/|x-y|^\alpha \right\} = 0, \forall x\neq y \in [a, b]$. And this implies that $|f(x)| = 0, \forall x\in[a, b]$, i.e. $f \equiv 0$. Conversely is obvious since $f = 0 \Rightarrow ||f||_\alpha = 0 $.

For any $k \in \mathbb R$, 
$$
\begin{aligned}
||kf||_\alpha & = \max\left\{ |kf(x)| + |kf(x) - kf(y)|/|x-y|^\alpha \right\}\\
 & = |k|\max\left\{ |f(x)| + |f(x) - f(y)|/|x-y|^\alpha \right\} \\
& = |k|\cdot ||f||_\alpha.
\end{aligned}
$$ 

To check the triangle inequality, take $f, g\in C^\alpha([a, b])$, 

$$
\begin{aligned}
||f+g||_\alpha & = \max\left\{ |f(x) + g(x)| + |f(x) + g(x) - f(y) - g(y)|/|x-y|^\alpha \right\} \\
& \leqslant \max\left\{ |f(x)| + |f(x) - f(y)|/|x-y|^\alpha + \max\left\{ |g(x)| + |g(x) - g(y)|/|x-y|^\alpha \right\}  \right\} \\
& \leqslant \ ||f||_\alpha + ||g||_\alpha.
\end{aligned}
$$

In conclusion, $\left(C^{\alpha} ([a, b]), ||\cdot||_\alpha\right)$ is a normed vector space.

\end{proof}



\item [(b)]

\begin{proof}

For any $f\in \overline{B}_\alpha(0, 1)$, $||f||_\alpha \leqslant 1$, and by definition, it implies $\sup_{x\in[a, b]}|f(x)| \leqslant 1$, which is uniformly bounded. And it also yields $|f(x) - f(y)| \leqslant |x-y|^\alpha, \forall x,y \in [a, b]$. Then, $f\in C([a, b])$ because $\forall \epsilon > 0$, we can let $\delta = \epsilon^{1/\alpha}$ such that $|f(x) - f(y)|< \delta^{\alpha} = \epsilon$, $\forall |x- y|<\delta$. Hence $\overline{B}_\alpha(0, 1) \subset C([a, b])$. This also yields that $\overline{B}_\alpha(0, 1)$ is equicontinuous.

Next we would like to show that for any sequence $\{f_n\}\subset \overline{B}_\alpha(0, 1)$, it will converge in $\overline{B}_\alpha(0, 1)$. Note that $[a, b]$ is a compact set on $\mathbb R$ and $\{f_n\}\subset \overline{B}_\alpha(0, 1)\subset C([a,b])$ is uniformly bounded and $\{f_n\}$ is equicontinuous, by Azel\`a -Ascoli Theorem, we know $\exists \{f_{n_k}\}\subset \{f_n\}$ converges to $\bar f \in \overline{B}_\alpha(0, 1)$. Hence, $\overline{B}_\alpha(0, 1)$ is compact in $\left(C([a, b]), ||\cdot||_\infty  \right)$.







\end{proof}



\end{enumerate}




\section*{Problem 8}

\begin{proof}

It is enough to show that $T$ is a contraction mapping. Then from Contraction Mapping principle, $T$ has a fixed point.

Suppose $T: X \rightarrow X$ is not a contraction mapping. Hence, take two different elements $x, y\in X$, for any $0<c<1$, $d(T(x), T(y)) \geqslant c d(x, y)$. Take sequences $\{x_n\}$ and $\{y_n\}$ from $X$ and they both have convergent subsequences $\{x_{n_k}\} \rightarrow \bar x$ and $\{y_{n_k}\} \rightarrow \bar y$. Then $\forall n \in \mathbb N$, 


\begin{equation}\label{p81}
d(T(x_n), T(y_n)) \geqslant (1-\frac{1}{n}) d(x_n, y_n).
\end{equation}

Next ,we need to show that $\lim_{n\rightarrow +\infty} T(x_n) = T(\bar x)$. Indeed, $\forall \epsilon >0$, $\exists N>0$ such that $d(T(x_n), T(\bar x)) < d(x_n, \bar x) < \epsilon$. Hence, take limits on both sides of (\ref{p81}) and we get $d(T(\bar x), T(\bar y)) \geqslant d(\bar x, \bar y)$. This yields a contradiction to the statement.

In conclusion, the claim is proved.

\end{proof}

\vspace{60mm}


\section*{Problem 9}

\begin{proof}

let $g(x) = \frac{1}{4} e^{f(x)}$. We need to show that $g: \mathbb{R} \rightarrow \mathbb{R}$ is a contraction map. If so, then there exists a unique $x_0\in\mathbb{R}$ such that $g(x_0) = x_0$, which is the unique solution to the equation.

For any $x\in \mathbb{R}$, $f(x) \in [0, 1]$. And also, let $h(z) = e^z, z\in [0, 1]$ and the derivative of $h$ is in $[1, e]$. Thus, there exists $0<c<1$, such that

$$
|g(x) - g(y)| = \frac{1}{4}|e^{f(x)} - e^{f(y)}| \leqslant \frac{1}{4}\max |h^\prime|\cdot |f(x) - f(y)| \leqslant \frac{e}{4}c|x-y|.
$$

Let $c^\prime = \frac{e}{4}c$ and note that $c^\prime \in (0, 1)$. Hence, $g$ is a contraction map.


\end{proof}

\section*{Problem 10}
\begin{enumerate}
\item
\begin{proof}

If $\alpha_1 = \alpha_2 = 0$, then the claim is trivial. If not, then we only need to show that there exists a positive $\beta $ such that

$$
\frac{||\alpha_1e_1 + \alpha_2e_2||}{|\alpha_1|+|\alpha_2|} \geqslant \beta.
$$

Let function $f:\mathbb{R}^2 \rightarrow \mathbb{R}$ be $f(c_1, c_2) = ||c_1e_1 + c_2e_2||$ and $\text{dom}f = \{(c_1, c_2)\in\mathbb{R}^2 | |c_1| + |c_2| = 1 \}$. It is clear that $f$ is continuous on $\mathbb R^2$ and dom$f$ is compact. So $f$ reaches its minimum $K\geqslant 0$ on dom$f$. Also, $K > 0$, because if $K = 0$ then $c_1 = c_2 = 0$, which leads to a contradiction that $(c_1, c_2) \notin \text{dom} f$. Hence, we have 
$$
\frac{||\alpha_1e_1 + \alpha_2e_2||}{|\alpha_1|+|\alpha_2|} = \left\| \frac{\alpha_1}{|\alpha_1| + |\alpha_2|}e_1 + \frac{\alpha_2}{|\alpha_1| + |\alpha_2|}e_2 \right\| \geqslant K.
$$

And this proves the claim.

\end{proof}

\item

\begin{proof}

Similar to the previous one, only need to show that there exists $\beta > 0$ such that
 
$$
\frac{||\sum_{i=1}^n \alpha_ix_i||}{\sum_{i=1}^n|\alpha_i|} \geqslant \beta.
$$

Let function $g:\mathbb{R}^n \rightarrow \mathbb{R}$ be $g(c) = ||\sum_{i=1}^n c_ie_i||$ and $\text{dom} g = \{c \in\mathbb{R}^n | \sum_{i=1}^n|c_i| = 1 \}$. It is clear that $g$ is continuous on $\mathbb R^n$ and dom $g$ is compact. So $g$ reaches its minimum $\kappa \geqslant 0$ on dom $g$. Also, $\kappa \notin 0$ with the same reason above. Hence, we have 
$$
\frac{||\sum_{i=1}^n \alpha_ie_i||}{\sum_{i=1}^n|\alpha_i|} = \left\|  \frac{\sum_{i=1}^n\alpha_i}{\sum_{i=1}^n |\alpha_i|}e_i \right\| \geqslant \kappa.
$$

And this proves the claim.


\end{proof}


\end{enumerate}






\end{document}