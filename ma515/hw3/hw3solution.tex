\documentclass[12pt]{article}
 \usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,amsfonts}
\usepackage{color}
\usepackage{mathtools, eucal}
\usepackage{xparse}
\usepackage{romannum}
 
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

 
\begin{document}
 
%\renewcommand{\qedsymbol}{\filledbox}
%Good resources for looking up how to do stuff:
%Binary operators: http://www.access2science.com/latex/Binary.html
%General help: http://en.wikibooks.org/wiki/LaTeX/Mathematics
%Or just google stuff
 
\title{MA 515 Homework 3}
\author{Zheming Gao}
\maketitle

\section*{Problem 1}

\begin{enumerate}
\item [(a)]

It is not a norm because it violates property (\romannum{2}). Let $a = -1, x = 1$. Then $||ax|| = ||-1|| = 2$. However, $|a|||x|| = 1$.

\item [(b)]
It is a norm.

\begin{proof}

Check the first one. If $||f|| = 0$, then $\sup_{t\geqslant 0} e^{\lambda t}|f(t)| = 0$. Since for any $t\geqslant 0$, $e^{\lambda t}|f(t)| \geqslant 0$ and $e^{\lambda t} > 0$, we have  $|f(t)| = 0, \forall t\geqslant 0$. This is to say $f \equiv 0$ on its domain. For the other direction, $||f|| = 0$ is true when $f = 0$.

For (\romannum 2), $\forall \alpha \in \mathbb{R}$,

$$
||\alpha x|| = \sup_{t\geqslant 0} e^{\lambda t}|\alpha f(t)| = \sup_{t\geqslant 0} e^{\lambda t}|\alpha|\cdot|f(t)| = |\alpha| \sup_{t\geqslant 0} e^{\lambda t}|f(t)| = |\alpha|\cdot ||x||.
$$

To check the triangle inequality, we pick $f, g \in X$,

$$
\begin{aligned}
||f + g|| = \sup_{t\geqslant 0} e^{\lambda t}|f(t) + g(t)| & \leqslant \sup_{t\geqslant 0} e^{\lambda t}\left(|f(t)| + |g(t)|\right)\\
& \leqslant \sup_{t\geqslant 0} e^{\lambda t}|f(t)| + \sup_{t\geqslant 0} e^{\lambda t}|g(t)| = ||f|| + ||g||.
\end{aligned}
$$

In conclusion, it is a norm.

\end{proof}

\item [(c)]

It is a norm.

\begin{proof}

We have shown that for $\ell^p$ space, $||x||_p = (\sum_{i=1}^{+\infty} |x_i|^p)^{1/p} $ is a norm for $1\leqslant p \leqslant +\infty$. Then, truncate it for only first two entries. Consider set $S = \{ x \in \ell^p \| x = (x_1, x_2, 0, \dots),  x_1, x_2 \in \mathbb{R}\}$. We have $||x||_p = ||x||, \forall x\in S$. Since $||\cdot||_p$ is a norm on $\ell^p$, it must be a norm on $S\subset \ell^p$ as $0\in S$. Then we conclude that $||\cdot||$ is a norm on $\mathbb{R}^2$.

\end{proof}

\item [(d)]

It is not a norm. Consider $x = (1, 0), y = (0, 1)$. Then $||x+y|| = ||(1,1)|| = 2^{1/p} > 2$. However, $||x|| + ||y|| = 2 < ||x + y||$. This breaks the triangle inequality.


\end{enumerate}

\section*{Problem 2}

\begin{proof}
\ 

\begin{enumerate}
\item [Step 1]

We are going to show $||(\cdot, \cdot)||$ is a norm on $X\times Y$. 

If $||(x, y)|| = 0$, then $\max \{ ||x||_X, ||y||_Y \} = 0$, which implies that $||x||_X = ||y||_Y = 0$. Hence, $x = y = 0$. The other direction is obvious. 

To check the second property, take arbitrarily $\alpha \in \mathbb R$ and we have 

$$
\begin{aligned}
||\alpha (x, y)|| = ||(\alpha x, \alpha y)|| & = \max \{ ||\alpha x||_X, ||\alpha y||_Y \}\\
& = \max \{ |\alpha | \cdot ||x||_X, |\alpha | \cdot ||y||_Y \}\\ & = |\alpha|\max \{ ||x||_X, ||y||_Y \} = |\alpha| \cdot ||(x, y)||.
\end{aligned}
$$

For triangle inequality, take arbitrarily $(x, y), (z, w) \in X\times Y$, we have 

$$
\begin{aligned}
||(x, y) + (z, w)|| = ||(x+z, y+w)|| & = \max\{ ||x+z||_X, ||y+w||_Y \} \\
& \leqslant \max\{ ||x||_X + ||z||_X, ||y||_Y + ||w||_Y \} \\
& = \max\{ ||x+||_X, ||y||_Y \} + \max\{ ||z||_X, ||w||_Y \} \\
& = ||(x, y)|| + ||(z, w)||.
\end{aligned}
$$

Hence, $||(\cdot, \cdot)||$ is a norm on $X\times Y$. 

\item [Step 2]

We need to prove that $X\times Y$ is complete. Take any Cauchy sequence $\{z_n = (x_n, y_n)\}_{n\in\mathbb N} \subset X\times Y$. $\forall \epsilon >0$, there exists $N > 0$ such that 
$$ 
||z_n - z_m|| = ||(x_n, y_n) - (x_m, y_m)|| < \epsilon, \quad \forall n, m > N.
$$

This is equivalent to $\max\{||x_n - x_m||_X, ||y_n - y_m||_Y\} < \epsilon \forall n, m > N$ and also implies that 

$$
||x_n - x_m||_X < \epsilon, \quad ||y_n - y_m||_Y < \epsilon, 
\forall n, m > N.
$$

Hence we conclude that $\{x_n\}$ and $\{y_n\}$ are both Cauchy sequences on $X$ and $Y$ respectively. Since $(X, ||\cdot||_X)$ and $(Y, ||\cdot||_Y)$ are Banach spaces, $\{x_n\}$ converges on $X$ and $\{y_n\}$ converges on $Y$. Let $\lim_{n\rightarrow +\infty} x_n = x$, $\lim_{n\rightarrow +\infty} y_n = y$, and $z = (x, y)\in X\times Y$. Then,
$$ 
||z_n - z|| = ||(x_n, y_n) - (x, y)|| = \max\{||x_n - x||_X, ||y_n - y||_Y\}< \epsilon, \quad \forall n > N.
$$

Hence, $\{z_n\}$ converges to on $z\in X\times Y$. This proves the claim.




\end{enumerate}


\end{proof}


\section*{Problem 3}

\begin{enumerate}
\item [(a)]

\begin{proof}

Firstly, we need to show that $d_f$ is a metric on $X$. $\forall x, y\in X$, $d_f(x, y) = f(||x - y||_X) \geqslant 0$ holds due to the property of $f$. And $d_f(x, x) = f(0) = 0$. Also, $d_f(x, y) = d_f(y, x)$ is obvious since $||x - y||_X = ||y - x||_X$. For triangle inequality, we need to use the facts that $f$ is increasing and \romannum 2,
$$
\begin{aligned}
d_f(x, y) + d_f(y, z) & = f(||x-y||_X) + f(||y-z||_X)  \geqslant f(||x-y||_X + ||y-z||_X)\\
& \geqslant f(||x-y + y-z||_X) = f(||x-z||_X) = d_f(x, z).
\end{aligned}
$$
Hence, $d_f$ is a metric well-defined on $X$.

Next, we need to show $(X, d_f)$ is complete. Take any Cauchy sequence $\{x_n\}_n\in\mathbb N$ from $(X, d_f)$ and it is enough to show it converges in $X$. $\forall \epsilon > 0$, there exists $N > 0$ such that $d_f(x_n, x_m) = f(||x_n - x_m||_X) < \epsilon$, for all $n, m > N$. Since $f$ is an incr easing continuous function from $\mathbb R_+$ to $\mathbb R_+$, then there exists an increasing continuous inverse function $f^{-1}$ of $f$ such that $f^{-1}(d_f(x, y)) = ||x - y||_X$. Hence,

$$
||x_m - x_n||_X = f^{-1}(d_f(x_m, x_n)) < f^{-1}(\epsilon) \rightarrow 0,\  \text{as} \quad \epsilon \rightarrow 0, \forall n, m > N
$$

Hence, $\{x_n\}$ is also a Cauchy sequence on $(X, ||\cdot||_X)$ and so it converges in $X$ in norm $||\cdot||_X$. Let $x_n\rightarrow x\in X$ in $||\cdot||_X$. Then we have $\lim_{n\rightarrow +\infty } ||x_n - x||_X = 0$. With the continuity of $f$, we know $\lim_{n\rightarrow +\infty } f(||x_n - x||_X) = 0$. And this improves that $x_n\rightarrow x$ in distance $d_f$. Hence $\{x_n\}$ is convergent on $(X, d_f)$.

In conclusion, $(X, d_f)$ is a complete metric space.

\end{proof}

\item [(b)]

\begin{proof}

Take arbitrarily $x, y\in B_f(0, 1)$ and $\alpha\in(0, 1)$. Then $z = \alpha x + (1- \alpha)y\in X$. It is enough to show that $d_f(0, z) < 1$. Use property (\romannum 2) and the property given in (b), we have the following,

$$
\begin{aligned}
d_f(0, z) = f(||z||_X) & \leqslant f(\alpha||x||_X + (1-\alpha)||y||_X) \\
& \leqslant f(\alpha||x||_X) + f((1-\alpha)||y||_X) \\
& \alpha f(||x||_X) + (1-\alpha)f(||y||_X)\\
& = \alpha d_f(0, x) + (1-\alpha) d_f(0, y) < \alpha + 1 - \alpha = 1.
\end{aligned}
$$ 

This proves that $z \in B_f(0,1)$ and then $B_f(0, 1)$ is convex.


\end{proof}


\end{enumerate}


\section*{Problem 4}


\vspace{60mm}


\section*{Problem 9}

\begin{proof}

let $g(x) = \frac{1}{4} e^{f(x)}$. We need to show that $g: \mathbb{R} \rightarrow \mathbb{R}$ is a contraction map. If so, then there exists a unique $x_0\in\mathbb{R}$ such that $g(x_0) = x_0$, which is the unique solution to the equation.

For any $x\in \mathbb{R}$, $f(x) \in [0, 1]$. And also, let $h(z) = e^z, z\in [0, 1]$ and the derivative of $h$ is in $[1, e]$. Thus, there exists $0<c<1$, such that

$$
|g(x) - g(y)| = \frac{1}{4}|e^{f(x)} - e^{f(y)}| \leqslant \frac{1}{4}\max |h^\prime|\cdot |f(x) - f(y)| \leqslant \frac{e}{4}c|x-y|.
$$

Let $c^\prime = \frac{e}{4}c$ and note that $c^\prime \in (0, 1)$. Hence, $g$ is a contraction map.


\end{proof}

\section*{Problem 10}
\begin{enumerate}
\item
\begin{proof}

If $\alpha_1 = \alpha_2 = 0$, then the claim is trivial. If not, then we only need to show that there exists a positive $\beta $ such that

$$
\frac{||\alpha_1e_1 + \alpha_2e_2||}{|\alpha_1|+|\alpha_2|} \geqslant \beta.
$$

Let function $f:\mathbb{R}^2 \rightarrow \mathbb{R}$ be $f(c_1, c_2) = ||c_1e_1 + c_2e_2||$ and $\text{dom}f = \{(c_1, c_2)\in\mathbb{R}^2 | |c_1| + |c_2| = 1 \}$. It is clear that $f$ is continuous on $\mathbb R^2$ and dom$f$ is compact. So $f$ reaches its minimum $K\geqslant 0$ on dom$f$. Also, $K > 0$, because if $K = 0$ then $c_1 = c_2 = 0$, which leads to a contradiction that $(c_1, c_2) \notin \text{dom} f$. Hence, we have 
$$
\frac{||\alpha_1e_1 + \alpha_2e_2||}{|\alpha_1|+|\alpha_2|} = \left\| \frac{\alpha_1}{|\alpha_1| + |\alpha_2|}e_1 + \frac{\alpha_2}{|\alpha_1| + |\alpha_2|}e_2 \right\| \geqslant K.
$$

And this proves the claim.

\end{proof}

\item

\begin{proof}

Similar to the previous one, only need to show that there exists $\beta > 0$ such that
 
$$
\frac{||\sum_{i=1}^n \alpha_ix_i||}{\sum_{i=1}^n|\alpha_i|} \geqslant \beta.
$$

Let function $g:\mathbb{R}^n \rightarrow \mathbb{R}$ be $g(c) = ||\sum_{i=1}^n c_ie_i||$ and $\text{dom} g = \{c \in\mathbb{R}^n | \sum_{i=1}^n|c_i| = 1 \}$. It is clear that $g$ is continuous on $\mathbb R^n$ and dom $g$ is compact. So $g$ reaches its minimum $\kappa \geqslant 0$ on dom $g$. Also, $\kappa \notin 0$ with the same reason above. Hence, we have 
$$
\frac{||\sum_{i=1}^n \alpha_ie_i||}{\sum_{i=1}^n|\alpha_i|} = \left\|  \frac{\sum_{i=1}^n\alpha_i}{\sum_{i=1}^n |\alpha_i|}e_i \right\| \geqslant \kappa.
$$

And this proves the claim.


\end{proof}


\end{enumerate}






\end{document}