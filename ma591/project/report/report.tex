\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,amsfonts}
\usepackage{enumitem}
\usepackage{placeins}
\usepackage{algorithm}
\usepackage{mathtools, eucal}
\usepackage{graphicx}
\usepackage{color}
\usepackage[noend]{algpseudocode}
 \newtheorem{theorem}{Theorem}
\def\bf{\textbf}
\def\BState{\State\hskip-\ALG@thistlm}

\begin{document}
 
%\renewcommand{\qedsymbol}{\filledbox}
%Good resources for looking up how to do stuff:
%Binary operators: http://www.access2science.com/latex/Binary.html
%General help: http://en.wikibooks.org/wiki/LaTeX/Mathematics
%Or just google stuff
 
\title{Project Report}
\author{Melissa Gaddy, Zheming Gao}
\maketitle

\section{Matrix Multiplicative Weights Algorithm and its application}



\subsection{Matrix MW Algorithm}

The motivation of Matrix MW Algorithm is similar to MW algorithm introduced in the preceding section.

Imagine a 2-player zero-sum game. The first player chooses a unit vector $\bf v$, and the second player chooses a matrix $M$ such that $\textbf{0} \preceq M \preceq I$. The second player has to pay $\bf v^TM\bf v$ to the first.We allow the first player to choose the vector $v$ from distribution $\mathcal D$ over the unit ball. In this case, we are interested in studying the value

$$
\mathbb E_{\mathcal D} \left[(\bf v^\top M\bf v)\right] = M \bullet \mathbb E_{\mathcal D} \left[\bf v\bf v^\top\right]
$$

Now the matrix $P:= \mathbb E_{\mathcal D} \left[\bf v\bf v^\top\right]$ is a density matrix, which has properties of positive semidefinite and trace 1.

As in MW algorithm, we consider an online 2-player zero-sum game. At each round the first player has to react to the opponent who picks matrix $M$. The matrix $M$ is called an \textit{observed event}. An online algorithm for first player to observe the event $M^{(t)}$ and choose a density matrix $P^{(t)}$ at each round $t = 1, \dots, T$. After $T$ rounds the best fixed vector $\bf v$ is the one that maximize the total payoff $\sum_{t = 1}^T \bf v^\top M \bf v$. 

For a symmetric matrix $A\in \mathbb R^n$, we denote its eigenvalues in an order as $\lambda_1(A) \geqslant \cdots \geqslant \lambda_n(A)$. Then it is obvious to see that the best maximizer $\bf v$ is the unit eigenvector of the largest eigenvalue of $\sum_{t = 1}^T M^{(t)}$, i.e., $\lambda_1(\sum_{t = 1}^T M^{(t)})$. The goal of this online algorithm is to find a solution vector $\bf v$ such that the value of $\bf v^\top M \bf v$ is not much less than the smallest expected value. We won't know which fixed vector $\bf v$ will be the best until we finish the last round. Hence, this algorithm has the advantage to make decisions while the game is playing.

Analogously, the Matrix MW algorithm is the following.

\begin{algorithm}\label{MMWoriginal}
   \caption{Matrix Multiplicative Weights Algorithm}
    \begin{algorithmic}[]
      \State Fix an $\epsilon < \frac{1}{2}$, and let $\eta = -\ln(1-\epsilon)$. 
    \For{$t = 1, 2, \dots, T$ }
    \State 1. Compute weight matrix $W^{(t)} = \exp(\eta\sum_{\tau =1 }^{t-1} M^{(\tau)})$.
    \State 2. Compute density matrix $P^{(t)} = \frac{W^{(t)}}{Tr(W^{(t)})}$.
    \State 3. Observe the event $M^{(t)}$.
     \EndFor
\end{algorithmic}
\end{algorithm}

\FloatBarrier

\subsection{Solving SDP with Matrix MW Algorithm}
We've introduced Multiplicative Weights(MW) algorithm and apply it on solving Linear programming problems of some certain form. We refer to the setting as \textit{basic} or  \textit{scalar} case. Now we are considering some problem which has enough structure that we can obtain an analogous algorithm for them.

Moving from vectors to matrices, and from probability vector to density matrix, now we refer to the current setting as \textit{matrix} case. 

\begin{minipage}{0.5\linewidth}


\begin{align*}
a_j^T \textbf x \geqslant 0 \\
\mathbf 1^T\textbf x = 1 \\
\textbf x\geqslant 0.
\end{align*}

\end{minipage}
\begin{minipage}{0.5\linewidth}

\begin{align*}
A_j \bullet X \geqslant 0 \\
Tr(X) = 1 \\
X \succeq 0.
\end{align*}

\end{minipage}

Notice that the feasibility problem on right hand side is a SDP problem. We are interested in using Matrix MW Algorithm to solve SDP problems of this certain form. As before, to fit the algorithm on this problem, we need to slightly modify Matrix MW Algorithm. Let parameter $\rho = \max_j\|A_j\|$. At each round $t$, set the observe event (gain matrix) $M^{(t)} = A_j/\rho$. 

\begin{algorithm}\label{MMWfeasibility}
   \caption{Matrix Multiplicative Weights Algorithm - SDP}
    \begin{algorithmic}[]
      \State Fix an $\epsilon < \frac{1}{2}$, and let $\eta = -\ln(1-\epsilon)$. $\rho = \max_j\|A_j\|$. 
      \State Initialize $W^{(0)} = I$, $X^{(0)} = \frac{1}{n}I$.
    \While{$A_j \bullet X^{(t)} < 0$}
    \State 1. Observe gain matrix $M^{(t)} = \frac{A_j}{\rho}$.
    
    \State 2. Compute weight matrix $W^{(t)} = \exp(\eta\sum_{\tau =1 }^t M^{(\tau)})$.
    \State 3. Compute density matrix $X^{(t)} = \frac{W^{(t)}}{Tr(W^{(t)})}$.
     \EndWhile
\end{algorithmic}
\end{algorithm}

The Matrix Multiplicative Weights Algorithm - SDP gives an lower bound of the gain in total.

\begin{theorem}

The Matrix Multiplicative Weights algorithm generates $X^{(t)}$, $t = 1, \dots, T$ such that:

$$
\sum_{t=1}^TM^{(t)} \bullet X^{(t)} \geqslant (1 + \epsilon) \lambda_1\left(\sum_{t=1}^T M^{(t)}\right) - \frac{\ln n}{\epsilon} 
$$
\end{theorem}

(Add reference, A Combinatorial, Primal-Dual approach)

**************************************************************************


\subsection{Max Cut problem}

The max cut problem can be reformulate as the following form

\begin{equation}\label{Originalproblem}
\begin{array}{c c c c}
\max & \frac{1}{4} L \bullet X &  & \\
\text{s.t.} & (e_je_j^T) \bullet X  & =  1,  & \forall j = 1,\dots, n \\ 
 & X  & \succeq  0. &
\end{array}
\end{equation}

where $X\in \mathbb S_+^n$.

If we denote the optimal value as $b$ and scale $X$ by $1/n$, then we would like to check the feasibility of the constraints: 


\begin{equation*}
\begin{array}{c c c c}
& \frac{n}{4b} L \bullet X & \geqslant 1 & \\
& n(e_je_j^T) \bullet X  & =  1,  & \forall j = 1,\dots, n \\ 
 & X  & \succeq  0. &
\end{array}
\end{equation*}

Finally, if we take $A_j = n(e_je_j^T) - I, (j = 1,\dots, n)$, and $A_{n+1} = \frac{n}{4b} L - I$, then the feasibility problem can be reformed as 

\begin{equation}\label{feasiblilityproblem}
\begin{array}{c c c c}
& A_j \bullet X & \geqslant 0 & \forall j = 1,\dots, n+1 \\
& \text{Tr}(X) & = 1 &\\ 
 & X  & \succeq  0. &
\end{array}
\end{equation}

on which the Matrix-MW algorithm can be applied. To apply Matrix-MW, we would like to solve the large-margin problem with an error parameter $\delta >0$. Denote $K = \{X \succeq 0 | \text{Tr}(X) = 1\}$, we either solve the problem to an additive error of $\delta$, i.e., find $X \in K$ such that $A_j \bullet X  \geqslant -\delta,  \forall j = 1,\dots, n+1$, or prove the system (\ref{feasiblilityproblem}) is infeasible.


Initialize $X^{(1)} = 1/n I$, and set parameters $\epsilon$, $\eta = -\ln(1-\epsilon)$, we apply Matrix-MW on three different examples. For each example, we list the result and errors for different parameters.




\subsubsection{Results}
\begin{table}[htbp]\label{toytable}
\centering
\begin{tabular}{||c|c|c|c|c||}
\hline
$\epsilon$ & 0.1 & 0.01 & 0.001 & 0.0001 \\
%\hline
%$\eta$ & 0.1054 & 0.0101 & 0.0010 & 0.0001 \\
\hline
$\delta$ & 0.1 & 0.01 & 0.001 & 0.0001 \\
\hline
$\text{\# of rounds}$ & 20 & 328 & 3465  & 34827 \\
\hline
$\text{Rank} (X)$ & 4 & 4 & 4 & 4 \\
\hline
$\|X - X^*\|_2$ & 2.7700 & 2.7557 & 2.7556 & 2.7545 \\
\hline
$\|X - X^*\|_2 / \|X^*\|_2$ & 0.6925   & 0.6889 & 0.6887 & 0.6886\\
\hline
$|b^* - b|$ & 0.4595 & 0.0482 & 0.0039 & 0.0002 \\
\hline
\end{tabular}
\caption{toy example}
\end{table}

\begin{table}[htbp]\label{10nodestable}
\centering

\begin{tabular}{||c|c|c|c|c||}
\hline
$\epsilon$ & 0.1 & 0.01 & 0.001 & 0.0001 \\
%\hline
%$\eta$ & 0.1054 & 0.0101 & 0.0010 & 0.0001 \\
\hline
$\delta$ & 0.1 & 0.01 & 0.001 & 0.0001 \\
\hline
$\text{\# of rounds}$ & 19 & 633 & 7230 & 73228 \\
\hline
$\text{Rank} (X)$ & 10 & 10 & 10 & 10 \\
\hline
$\|X - X^*\|_2$ & 8.9298 & 8.9171 & 8.9169 & 8.9168 \\
\hline
$\|X - X^*\|_2 / \|X^*\|_2$ & 0.8930 & 0.8917 & 0.8917 & 0.8917 \\
\hline
$|b^* - b|$ & 1.9107 & 0.1908 & 0.0204 & 0.0011 \\
\hline
\end{tabular}
\caption{10 nodes example}
\end{table}

\begin{table}[htbp]\label{100nodestable}
\centering
\begin{tabular}{||c|c|c|c|c||}
\hline
$\epsilon$ & 0.1 & 0.01 & 0.001 & 0.0001 \\
%\hline
%$\eta$ & 0.1054 & 0.0101 & 0.0010 & 0.0001 \\
\hline
$\delta$ & 0.1 & 0.01 & 0.001 & 0.0001 \\
\hline
$\text{\# of rounds}$ & 2777 & 49971 &  & \\
\hline
$|b^* - b|$ & 3.2430 & 0.7318 &  & \\
\hline
\end{tabular}
\caption{100 nodes example}
\end{table}



\subsubsection{Comments}

\begin{enumerate}
\item
"\# or rounds" is the number of iterations for Matrix-MW algorithm. It stops when $A_j \bullet X  \geqslant -\delta$ is satisfied for any $j = 1, \dots, n+1$. 

\item
Solution $X$ and the value of $b = \frac{1}{4} L \bullet X$ are obtained from iteration. $X^*$ and $b^*$ are optimal solution and optimal value to problem (\ref{Originalproblem}). We list the error of solution $\|X- X^*\|_2$ and the error of object value $|b - b^*|$.

\item Notice the solution absolute error in 2-norm is very large, so we also calculate the relative error in 2-norm for the toy example and the 10-node example.


\item
The 100-node example has blank units in table since the algorithm converges very slow when $\epsilon$ and $\delta$ are smaller than $10^{-2}$. (\# of iteration will exceed $10^{5}$ ). 

\item
Rank of the iteration solution is not 1. So the Rank = 1 constraint of original MAX CUT problem cannot be eliminated.


\item 
Notice that for each result, the $\epsilon$ and $\delta$ are in the same order of magnitude. The reason is, if we set $\epsilon$ and $\delta$ in different orders of magnitude, e.g., $\epsilon = 0.01$ and $\delta = 0.001$, or $\epsilon = 0.001$ and $\delta = 0.01$, the algorithm will not converge or converge very slow. From this we conclude that Matrix-MW is very parameter sensitive.

\end{enumerate}




\end{document}