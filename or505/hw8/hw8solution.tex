\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,amsfonts, }
\usepackage{enumitem}
\usepackage{placeins}
\usepackage{mathtools, eucal}
\usepackage{graphicx}
\usepackage{color}
\usepackage{subcaption}

\usepackage{amsmath,amsgen,amstext,amsbsy,amsopn,amsfonts,amsthm}
% standard AMS packages for symbols, formats, etc.
\usepackage{graphicx,tabularx,array,geometry,float}
%\usepackage{hyperref}
% packages for web interface, graphics, etc,

 
\newtheorem{remark}{Remark}
 
\begin{document}
 
%\renewcommand{\qedsymbol}{\filledbox}
%Good resources for looking up how to do stuff:
%Binary operators: http://www.access2science.com/latex/Binary.html
%General help: http://en.wikibooks.org/wiki/LaTeX/Mathematics
%Or just google stuff
 
\title{Homework 8 Solutions}
\author{Fangroup}
\maketitle

\section*{Problem 1}

\begin{enumerate}
\item [(a)]

\begin{equation*}
\begin{array}{rl}
\text{(Primal):} & \\
\min & 0 \\\text{s.t.} & x_1 + x_2 =1 \\
& 2x_1 + x_2 = 3
\end{array} \qquad \qquad \qquad
\begin{array}{rl}
\text{(Dual):} & \\
\max & w_1 + 3w_2 \\
\text{s.t.} & w_1 + 2w_2 = 0 \\
& w_1 + w_2 = 0
\end{array}
\end{equation*}

Both primal and dual problems have bounded non-empty feasible domains(singletons).

\item [(b)]

\begin{equation*}
\begin{array}{rl}
\text{(Primal):} & \\
\min & -x_1 + x_2 \\\text{s.t.} & x_1 + x_2 \leqslant 1 \\
& x_1 \geqslant 0, x_2 \geqslant 0.
\end{array} \qquad \qquad \qquad
\begin{array}{rl}
\text{(Dual):} & \\
\max & w \\
\text{s.t.} & w \leqslant -1 \\
& w \leqslant 1 \\
& w \leqslant 0
\end{array}
\end{equation*}

Dual feasible domain is unbounded. 

\item [(c)]

\begin{equation*}
\begin{array}{rl}
\text{(Primal):} & \\
\min & -x_1 + x_2 \\\text{s.t.} & x_1 + x_2 \geqslant 1 \\
& x_1 + x_2 \leqslant -1
\end{array} \qquad \qquad \qquad
\begin{array}{rl}
\text{(Dual):} & \\
\max & w_1 - w_2 \\
\text{s.t.} & w_1 + w_2 = -1 \\
& w_1 + w_2 = 1\\
& w_1 \geqslant0, w_2 \leqslant 0
\end{array}
\end{equation*}

Both primal and dual problems are infeasible.

\item [(d)]

\begin{equation*}
\begin{array}{rl}
\text{(Primal):} & \\
\min & -x_1 + x_2 \\\text{s.t.} & x_1 + x_2 \geqslant 1 \\
& x_1 \geqslant 0, x_2 \geqslant 0.
\end{array} \qquad \qquad \qquad
\begin{array}{rl}
\text{(Dual):} & \\
\max & w \\
\text{s.t.} & w \leqslant -1 \\
& w \leqslant 1 \\
& w \geqslant 0
\end{array}
\end{equation*}

Primal feasible domain is unbounded, but the dual is infeasible.

\end{enumerate}

\section*{Problem 2}

\begin{proof}

Consider a standard form LP. ($A\in \mathbb R^{m\times n}$) If the LP has multiple optimal solutions, the reduced cost for each optimal basis $B$ will be 

$$
r^T = c^T - c_B^T B^{-1} A = c^T - w^T A.
$$

where $w$ is the correspoinding dual optimal solution. $r$ has more than $m$ zero elements. This is to say, in the system 

$$
A^Tw \leqslant c
$$

more than $m$ equalities hold. Hence, the dual problem is degenerate.

\end{proof}

\section*{Problem 3 (4.10)}

\begin{enumerate}
\item[(a)]
Let the objective function be the total cost of shipping. Since we've assumed the total amount of available units equals the total amount required, the LP problem is formed as

	\begin{equation*}
	\begin{array}{rl}
	\min & \sum_{i=1}^m\sum_{j=1}^n c_{ij}x_{ij} \\
	\text{s.t.} & \sum_{j=1}^n x_{ij} = a_i \quad (\forall i = 1, \dots, m) \\
	& \sum_{i=1}^m x_{ij} = b_j \quad (\forall j = 1, \dots, n )\\
	& x_{ij} \geq 0, (\forall i = 1, \dots, m) (\forall j = 1, \dots, n )
	\end{array}
	\end{equation*}
\item[(b)]
Note that we have $m+n$ constraints in the primal problem. Let $u_i, (i = 1, \dots, m)$ be dual variables corresponding to first $m$ constraints, and $v_j, (j = 1, \dots, n)$ be dual variables corresponding to the rest. Then the dual problem is,

	\begin{equation*}
	\begin{array}{rl}
	\max & \sum_{i=1}^m a_iu_i + \sum_{j=1}^n b_{j}v_{j} \\
	\text{s.t.} & u_i + v_j \leqslant c_{ij}  \\
	& (\forall i = 1, \dots, m) \\
	&(\forall j = 1, \dots, n )
	\end{array}
	\end{equation*}

\item[(c)]
Suppose $x^* = (x_11, \dots, x_{m1}, x_{12}, \dots, x_{mn})^T$ and $w^* = (u_1, \dots, u_m,v_1, \dots, v_n)^T$ are optimal solutions to the primal and the dual problem respectively. Then the complementary slackness is

\begin{equation*}
\begin{aligned}
&(b_j - \sum_{i=1}^m x_{ij})v_j = 0, \quad & \forall j = 1, \dots, n \\
&(a_i - \sum_{j=1}^n x_{ij})u_i = 0, \quad &\forall i = 1, \dots, m \\
&(c_{ij} - u_i - y_j)x_{ij} = 0, \quad &(\forall j = 1, \dots, n)\\
& & (\forall i = 1, \dots, m).
\end{aligned}
\end{equation*}

\item[(d)]

Given $w^* = (u_1, u_2, u_3, v_1, v_2, v_3, v_4)^T = (0, 3, -4, 7, 2, -5, 7)^T$. Plug it into the result of (c) and get the optimal solution to primal problem: (For convenience I list it as a matrix form. $x_{ij}$ is the $(i, j)$th element of matrix $X$)
$$
X = \begin{bmatrix}
2 & 1 & 0 & 0 \\
0 & 2 & 1 & 0 \\
0 & 0 & 1 & 3
\end{bmatrix}
$$

\end{enumerate}


\section*{Problem 4 (5.1)}

\begin{figure}[htbp]
  \caption{Figures of all functions}
  \centering
    \includegraphics[width=0.5\textwidth]{Fig1.jpg}
    \label{fig1}
\end{figure}


\begin{enumerate}
\item [(a)]

\begin{figure}[htbp]
  \caption{Part(a)}
  \centering
    \includegraphics[width=0.5\textwidth]{part_a.jpg}
    \label{part_a}
\end{figure}

From Fig.\ref{part_a} we can see that a quadratic method does not always perform better than a cubic algorithm. It depends on the scale of the problem. $f_2$ is not always less than $f_4$.

\item [(b)]

\begin{figure}[htbp]
  \caption{Part(b)}
  \centering
    \includegraphics[width=0.5\textwidth]{part_b.jpg}
    \label{part_b}
\end{figure}

Similarly, a polynomial algorithm doesn't always perform better than exponential algorithm. $f_2$ is not always less than $f_5$.

\end{enumerate}





\section*{Problem 5 (7.1)}



(Thanks to Dr. Nie for the solution)

\begin{itemize}
	\item[(a)] W.l.o.g, we assume that the linear programming problem is in the standard form \begin{align*}
	\min ~c^Tx~\quad\text{s.t.} ~Ax=b, ~x\geq 0,
	\end{align*}where $A\in\mathbb{R}^{m\times n}$, $c\in\mathbb{R}^n$ and $b\in\mathbb{R}^m$. Given a basis $B$, the corresponding basic solution can be obtained by solving the following system of linear equations: 
	\begin{align*}
	\left( \begin{array}{ll} B & N \\ 0 & I \end{array}\right) \left( \begin{array}{l}x_B\\x_N\end{array}\right) =\left( \begin{array}{l}b\\0\end{array}\right)
	\end{align*}
	Therefore, using Algorithm A, we are able to list all the basic solutions to this LP problem. Then, the LP problem can be solved by finding the basic feasible solution with minimal objective value.
	
	\item[(b)] Given a system of linear equations, we construct a linear programming problem whose objective function is constant and feasible domain is defined by those linear equations. Using Algorithm B, we are able to solve the LP problem, i.e., obtain an optimal solution. This optimal solution is feasible, and thus solves the system of linear equations. 
	\item[(c)] From (a) and (b), it is of the same difficulty (computational complexity) to solve systems of linear equations and linear programming problems, respectively. Since the systems of linear equations can be solved in polynomial time, we know that linear programming problems are polynomial-time solvable. 
\end{itemize}


\section*{Problem 6}

\begin{proof}

\begin{enumerate}
\item [(T1)]
$\forall \textbf x^k \in int(\mathbb R^n_+)$, i.e., $x^k_i >0, \forall i = 1, \dots, n$, matrix $X_k = diag(x^k_1, \dots, x^k_n)$ is invertible. Hence, $T_k(\textbf x) \in \mathbb R^n_+$. And it is also clear that $T(\textbf x) = T(\textbf y)$ when $ \textbf x = \textbf y$.

\item [(T2)]
Straight forward algebra. 

$$
T_k(\textbf x^k) = X_k^{-1}\textbf x^k = diag(x^k_1, \dots, x_n^k) \begin{pmatrix}
x^k_1 \\ \vdots \\ x^k_n
\end{pmatrix} = 
\begin{pmatrix}
1 \\ \vdots \\ 1
\end{pmatrix} = \textbf e.
$$

\item [(T3)]
The only vertex of $\mathbb R^n_+$ is the origin. And $T(\textbf 0)= \textbf 0$.

\item [(T4)]

Suppose $\textbf x \in bdry(\mathbb R^n_+)$. Hence, $\exists i$ such that $x_i = 0$. Then, the ith element of $T(\textbf x)$, denote as $y_i$, is $y_i = x_i/x^k_i = 0$. Hence, $T(\textbf x)\in bdry(\mathbb R^n_+)$.

\item [(T5)]

Suppose $\textbf x \in int(\mathbb R^n_+)$. i.e., $\forall i, x_i > 0$. Then, for any element of $T(\textbf x)$, denote as $y_i$, we have $y_i = x_i/x^k_i > 0, \forall i$. Hence, $T(\textbf x)\in int(\mathbb R^n_+)$.

\item [(T6)]
To show $T_k$ is one-to-one, we need to show that if $\textbf x \neq \textbf y \in \mathbb R^n_+$, then $T_k(\textbf x) \neq T(\textbf y)$. Indeed, if $\textbf x \neq \textbf y$, then $\exists j$ such that $x_j \neq y_j$ and so $x_j/x^k_j \neq y_j/x^k_j$. Hence, $T_k(\textbf x) \neq T(\textbf y)$.

$T_k$ is an onto mapping. Indeed, for each $\textbf z \in \mathbb R^n_+$, we can find $\textbf x = X_k \textbf z \in \mathbb R^n_+$ such that $T_k(\textbf x) = \textbf z$. 

Hence $T_k$ is a linear bijection and so it has inverse mapping. Let $T^{-1}_k(\textbf y)= X_k\textbf x$. Then we only need to check that $T_k\circ T_k^{-1} = T_k^{-1} \circ T_k = I$. It is obvious.
 


\end{enumerate}



\end{proof}




\end{document}